{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f28e6a-1e65-49cc-988e-8bca67ace3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Libraries\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "import pathlib\n",
    "import httpx\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8bece6-7bb9-432d-9aa1-717cead608f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parameters to change for your configuration\n",
    "\n",
    "# Year to extract data.  If you don't want to go by year, change pdf_path, file_pref, and output_path\n",
    "year = '2015'     \n",
    "\n",
    "# Model you want to use\n",
    "model = \"gemini-2.5-flash\"\n",
    "\n",
    "# API Key path\n",
    "api_path = \"API_Keys/google_gemini_key.txt\"\n",
    "\n",
    "# DAS path\n",
    "das_path = '../data/DAS/'+year+'_das.csv'\n",
    "\n",
    "# Output csv file path\n",
    "output_path = year+'_classification.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a11ed0b-90d6-45ee-9c84-b33da5d1a9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get file names and DAS's\n",
    "df = pd.read_csv(das_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba34514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert column names as the first row\n",
    "df.loc[-1] = df.columns  # add column names as a new row\n",
    "df.index = df.index + 1  # shift index\n",
    "df = df.sort_index()     # reorder rows\n",
    "\n",
    "# Rename columns \n",
    "df.columns = ['file_name', 'DAS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec74484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get api key for google gemini\n",
    "# This is an important step.  The key allows you to access the gemini api.\n",
    "# Keys should never be shared or hard coded into programs.\n",
    "# You can get a key by having a google account and clicking on Get API key here: https://aistudio.google.com/  (top right of the page)\n",
    "\n",
    "with open(api_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    google_api = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4974cd46-20a6-4857-8a28-a4284b616818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function with a prompt for the LLM.  Briefly the prompt looks at the DAS and categorizes it into 7 categories.\n",
    "\n",
    "client = genai.Client(api_key=google_api_key)\n",
    "def google_chat(data_statement):\n",
    "\n",
    "    prompt = \"\"\"\n",
    "    You are a classification assistant. Analyze the following data availability statement and determine which category or categories it fits best from the list below. \n",
    "    \n",
    "    Output only the corresponding number(s), separated by commas if more than one applies. Do not include any explanation or extra text.\n",
    "    \n",
    "    Categories:\n",
    "    \n",
    "    1. Data is included in the main text or supporting data is explicitly stated to be in the manuscript (e.g., “data are included in the article,” “data supporting findings are in the text,” “source data are provided”).\n",
    "    \n",
    "    2. Data are available upon request or by contacting the corresponding author (e.g., “data available from authors upon request,” “available from corresponding author”).\n",
    "    \n",
    "    3. Data are deposited in a public repository or database (e.g., GEO, Dryad, Figshare, OSF) with a link, DOI, or accession number.\n",
    "    \n",
    "    4. Data are available in the supplementary or supporting materials (e.g., “see Supplementary Information”).\n",
    "    \n",
    "    5. No data availability statement is included.\n",
    "    \n",
    "    6. Data sharing is not applicable (e.g., article type is a review, commentary, editorial, or no original data was generated).\n",
    "\n",
    "    7. Previously published data was used for this work. OR Source data for the paper was collected from a database. OR Mention of publicly used datasets.\n",
    "\n",
    "    Here are examples:\n",
    "\n",
    "    1. All study data are included in the main text.\n",
    "    2. The data that support the findings of this study are available from the corresponding author upon reasonable request.\n",
    "    2. Any data pertaining to this manuscript will be made available upon request.\n",
    "    3. Metabolomics, lipidomics, mass spectrometry, and RNA sequencing data have been deposited at National Metabolomics Data Repository (NMDR), ProteomeXchange Consortium, and GEO repository and are publicly available as of the date of this publication using the accession number provided in the key resources table. An Excel file includingsource data has been deposited in Mendeley Data and is publicly avail-able under the following site: DOI: 10.17632/nrpm8ydybr.1. This paper does not report original code.\n",
    "    4. See Supplement 2.\n",
    "    4. The data that supports the findings of this study are available in the supplementary material of this article.\n",
    "    5. There is no data availability statement.\n",
    "    6. There are no data in this manuscript to share.\n",
    "    6. Data sharing is not applicable to this article as no new data were created or analyzed in this study.\n",
    "    1,2  Data supporting the findings are in the main text. Other data is available upon request.\n",
    "    1,2,3 Sequence data from this article can be found in GEO The data supporting the findings and claims of this study is mentioned in the main text and is available with the corresponding author.\n",
    "    1,2,3,4 All data are available in the main text and the supplemental information or at public databases. Sequence data were deposited in Genome Sequence Archive. Any additional information required to reanalyze the data reported in this study is available from the lead contact upon request.\n",
    "    1,3 Cell line scDNA-seq (GSE270567) and scRNA-seq (GSE270568) are available through GEO. Patient scDNA-seq is available upon reasonable request. Image data is available upon reasonable request for cell lines and patients. If interested in using the High Definition Single Cell Assay please contact CSI-Cancer.\n",
    "    1,4 All data relevant to the study are included in the article or uploaded as online supplemental information. Data will be made available following acceptance.\n",
    "    1,2,4 All data are available in the main text and supplementary materials. All seed stocks generated in the study are freely available to the research community upon request.\n",
    "    1,3,4 All data generated or analyzed during this study are included in this published article, its supplementary information files, and publicly available repositories.\n",
    "    2,3 RNA-seq data have been deposited at GEO and are publicly available as of the date of publication. Accession number is listed in the key resources table. This paper does not report original code. Any additional information required to reanalyze the data reported in this paper is available from the lead contact upon request.\n",
    "    2,4 The original contributions presented in the study are included in the article/Supplementary material, further inquiries can be directed to the corresponding author.\n",
    "    2,3,4 The generated cryo-EM maps and PDB codes associated with different structures have been deposited in the EMDB and PDB databases, with the details mentioned in the key resources table. Raw data associated with the gel images and quantitative kinetics assays in Figures 1, 2, 5,6, 7, S1, S5, and S7 were deposited in Mendeley and are publicly avail-able at https://doi.org/10.17632/gzpdjrr8hj.1. This paper does not report original code. Any additional information required to reanalyze the data reported in this paper is available from the lead contact upon request.\n",
    "    3,4 Raw data and information for CRISPR-generated alleles, all quantifications, and exact P values (one-way ANOVA and Tukey test) are in Supplementary Table S6. The raw Sanger sequence traces for edited sequences are in Supplementary Data Files.The tomato and groundcherry BioProject accession numbers are PRJNA491365, PRJNA704671, and PRJNA862958.\n",
    "    7 The source data of this paper are collected in the following database record: biostudies:S-SCDT-10_1038-S44319-024-00304-5.\n",
    "    7 The cancer dependency and expression datasets were obtained online at https://depmap.org/portal/download/ (DepMap Public 21Q4).\n",
    "    7 Publicly used databases in study: NCBI Homologene, 11/22/2019, https://www.ncbi.nlm.nih.gov/homologene, GENCODE mm10 (v16), https://www.gencodegenes.org, JASPAR 2020 database, http://jaspar.genereg.net.\n",
    "    7 This paper analyzes existing, publicly available data.\n",
    "    2,3,7 This paper analyzes existing, publicly available data. The accession numbers for the datasets are listed in the key resources table. The full meta-marker lists for the BICCN cell types and optimal number of markers have been depositied on FigShare and are publicly available as of the date of publication. DOIs are listed in the key resources table. All original code has been deposited at Github at https://github.com/gillislab/MetaMarkers and is publicly available as of the date of publication. Any additional information required to reanalyze the data reported in this paper is available from the lead contact upon request.\n",
    "  \n",
    "    Data availability statement:\n",
    "    {context}\n",
    "    \n",
    "    \"\"\"\n",
    "    response = client.models.generate_content(\n",
    "      model=\"gemini-2.5-flash\",\n",
    "      contents=[data_statement,prompt])\n",
    "    # print(response.text)\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b4ab2f-3677-4281-8c49-eaeaed2542d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The server is sometimes very busy so sometimes you have to try multiple times to get the LLM to respond.\n",
    "# Here it will try five times to get the model to respond.  Each time it will wait longer to increase a chance of a response.\n",
    "\n",
    "def multi_try(data_statement):\n",
    "    for attempt in range(5):\n",
    "        try:\n",
    "            result = google_chat(data_statement)\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt+1} failed: {e}\")\n",
    "            wait = 2 ** attempt + random.random()\n",
    "            time.sleep(wait)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00d955d-e976-4ade-a1ce-46ca3ac72612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This sets up all of our functions to batch run multiple files.\n",
    "# Depending what model you choose it will only allow a certain number of requests a day.\n",
    "# The gemini-2.5-flash only allows 250 requests per day (RPD).\n",
    "\n",
    "classification = []  # classification output from the LLM\n",
    "loc = []             # find location of success\n",
    "loc_start = 0        # initialize\n",
    "fail = []            # fail das\n",
    "fail_loc = []        # fail location\n",
    "\n",
    "for das in df['DAS']:\n",
    "    try:\n",
    "        result = multi_try(das) # This will try multiple times to access the LLM. If it is busy, this does not count as a request as part of your requests per day.\n",
    "        classification.append(result)\n",
    "        loc.append(loc_start)\n",
    "        loc_start +=1\n",
    "        time.sleep(6)             # For the current model you are only allowed 10 request per minute.  The wait makes sure we stick to this.\n",
    "        print(loc_start,result)\n",
    "    except:\n",
    "        print('FAILURE!')  # If there is an error (e.g. can't find file or not response from LLM) it will print FAILURE.\n",
    "        fail.append(das)\n",
    "        fail_loc.append(loc_start)\n",
    "        loc_start +=1\n",
    "        classification.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa802d8-8b00-407a-a209-4577908bf7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['classification'] = classification # put the classification into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69068c40-6d90-4596-aff7-403dd777110e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(output_path) # save to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa89cd3-48fa-4bdd-8fbc-254898d6ae67",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(fail) # how many failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539731e5-33d9-45d3-84b3-a555633013ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "fail_loc # location of failed DASs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
